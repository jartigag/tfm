\chapter{Estado del Arte}\label{chap:estadodelarte}

En este capítulo se proporcionará un contexto adecuado para el trabajo, repasando el estado del arte que concierne al aprendizaje automático aplicado a clasificación de tráfico de red.
Se distinguirá entre enfoques basados en flujos y basados en hosts.
Después, se ahondará en la relevancia de la detección de anomalías dentro de la seguridad informática.
Ahí se contrapondrán las aproximaciones basadas en firmas y las basadas en comportamientos anómalos.
Entre las técnicas que se usan para estas últimas, se va a destacar el \emph{clustering}, en torno al cual gira este desarrollo.
Se relacionarán en la última sección algunos trabajos que aplican dicha técnica a la clasificación de tráfico.

\section{Aprendizaje automático en la clasificación de tráfico}\label{sec:aprendizajeautomaticoenclasiftrafico}

El aprendizaje automático puede resumirse como una colección de técnicas de gran potencial para la minería de datos y el descubrimiento de conocimiento \cite{NA08}.
En concreto, a la hora de extraer este conocimiento, estas técnicas son especialmente eficaces buscando y describiendo patrones estructurales útiles en los datos.
Además, la ventaja intrínseca de esta disciplina frente a la exploración que pueda efectuar un especialista es que, al poder definirse algorítmicamente el procedimiento para su aplicación, se hace posible implementar sistemas automatizados sobre equipos informáticos.

Un sistema de \emph{machine learning} aprende automáticamente de la experiencia y perfecciona su base de conocimiento, entendiendo ``aprender'' como la acción de mejorar su rendimiento en el desempeño de una tarea determinada \cite{Simon_1983}.
Esta mejora debe ser cuantificable objetivamente en base a lo que se denomina como medida de rendimiento.
El sistema (como cualquier otro agente computacional en la rama de la inteligencia artificial) recibirá estímulos externos (en este caso, conjuntos de datos) y,
en base a ellos y al conocimiento que recogen el algoritmo y los resultados de sus ejecuciones previas, producirá una salida que maximizará la medida de rendimiento establecida.

En terminología de \emph{machine learning}, el conjunto de datos que se toma como entrada se compone de instancias.
Una \emph{instancia} simboliza a un individuo específico de la población sobre la que se trabaja.
Cada instancia se representa por sus valores en una serie de \emph{características} o atributos, que no son más que medidas sobre aspectos de interés para el escenario en cuestión.
Un conjunto de instancias con ciertas características comunes pertenece a una \emph{clase} o concepto.
De este modo, se aprende un concepto cuando, dada una instancia, se logra identificar correctamente con qué clase se corresponde.
Este aprendizaje implica también que se es capaz tanto de generalizar la aplicación del nombre de la clase a todos los miembros de la misma como de discriminar a los miembros que pertenecen a otra clase.

Atendiendo a la naturaleza de las clases resultantes, los tipos de aprendizaje se dividen en aprendizaje supervisado y no supervisado.
El primer tipo es capaz de clasificar nuevas instancias en clases predefinidas.
Por el contrario, el segundo clasifica las instancias en clases no definidas con anterioridad.
La técnica de aprendizaje no supervisado principal es el \emph{clustering} o agrupamiento, que se explicará con detalle más adelante.

En los últimos tiempos, el aprendizaje automático se ha venido usando cada vez más en la clasificación de tráfico IP \cite{Dainotti_2012}.
Las técnicas de clasificación de tráfico siguen mejorando en acierto y eficiencia, pero
la proliferación constante de aplicaciones en Internet con comportamientos muy variados, sumado a
los incentivos que tienen ciertos agentes para enmascarar algunas aplicaciones y así evitar el filtrado o bloqueo en firewalls,
son algunas de las razones por las que la clasificación de tráfico permanece como uno de los muchos problemas abiertos de Internet.
Han quedado obsoletos métodos clásicos como la identificación de aplicaciones en base a sus puertos conocidos de nivel de transporte TCP/UDP (aquellos registrados por la IANA),
que resulta muy simple y rápida pero también poco fiable.
En el otro extremo, las técnicas de ``Deep Packet Inspection''\cite{Finamore_2011}, que analizan en profundidad el funcionamiento de las aplicaciones desde la perspectiva de su uso de los protocolos o
buscan datos específicos en paquetes IP para inferir a qué aplicación pertenecen, suponen una alta carga computacional y habitualmente requieren hardware específico.
Además, el buen funcionamiento de un clasificador DPI está supeditado a dos condiciones: que pueda inspeccionar el contenido de los paquetes IP y que sepa cómo interpretar la sintaxis de cada aplicación.
La primera condición queda comprometida por la estandarización de las conexiones cifradas,
mientras que la viabilidad de la segunda se vería restringida por la complejidad de contar con un repositorio completo y constantemente actualizado del formato de los paquetes que puede generar cada aplicación.
Ante estas técnicas también se presentan dificultades legales y relacionadas con la privacidad.

Si se trata de clasificadores de tráfico, lo más común es encontrar planteamientos basados en flujos de tráfico.
En ocasiones, la granularidad de la clasificación se afina hasta el uso de flujos bidireccionales (asumiendo que se tiene visibilidad de ambas direcciones), pero operar a este nivel entraña una complejidad bastante mayor.
Un flujo se suele definir como una tupla de 5 elementos: protocolo de transporte (frecuentemente, TCP o UDP), direcciones IP de origen y destino, y puertos de origen y destino.
Con este concepto como \emph{objeto} fundamental, tradicionalmente se han buscado patrones estadísticos en los atributos de los flujos que son observables desde una perspectiva externa (es decir, sin considerar el contenido o \emph{payload} de los paquetes).
Ejemplos de estos atributos serían: tamaño de paquetes, tiempo entre llegadas, número de paquetes en cada dirección, duración total, etc. resumido cada uno con el estadístico muestral que se considere adecuado.
Con la popularización del aprendizaje automático, se ha podido llevar la búsqueda de patrones entre dichos atributos a nuevos grados de profundidad.

Se trabaja también con otras variantes en cuanto a cómo agrupar los paquetes que se hayan intercambiado dos máquinas.
Entre ellas, podrían destacarse las conexiones TCP o los servicios, definidos estos como el tráfico generado entre una pareja de IPs-puertos.
En cualquiera de los casos anteriores, se pone el foco sobre flujos individuales, para después clasificarlos bajo categorías que comparten características.
Este tipo de planteamientos no tienen tan en cuenta el conjunto de acciones que lleva a cabo un mismo equipo.
Así, se corre el riesgo de perder información útil de cara a entender de forma completa qué es realmente lo que están haciendo los equipos de la red.

Por otro lado, se encuentran los clasificadores de tráfico basados en el comportamiento del host.
Sirva de referente el trabajo de \cite{KPF05}, donde se propuso un novedoso método que identificaba patrones en el comportamiento de los hosts a la altura de la capa de transporte.
Se trataba de una aproximación multinivel que descartaba incluir datos sobre el payload, los puertos bien conocidos (aspectos que conllevan las problemáticas anteriormente comentadas) o cualquier otra información separada de la que ofrecían los colectores de flujos.
Consistía por tanto en un clasificador ``a ciegas'' (\emph{``BLINd Classification'', abreviado como ``BLINC''}) que analizaba cada hosts desde tres perspectivas: social, funcional y aplicativa.
La perspectiva social capturaba las interacciones del host con otros hosts, en términos de cuántos hosts se conectaban con qué hosts.
La funcional los separaba según actuaran como proveedores de un servicio, consumidores o ambos.
Se tenían en cuenta, por tanto, los roles del modelo cliente-servidor.
Por último, en la perspectiva aplicativa se utilizaba la información por encima de la capa de transporte con la intención de distinguir la aplicación en cuestión.

Mediante la premisa de no tratar cada flujo como una entidad distinta, se conseguiría acumular la información necesaria para reconocer el verdadero comportamiento de cada equipo informático final.
Además de cumplir con la identificación de aplicaciones específicas, este método sería resistente a circunstancias de la red como congestión o cambios de rutas.
Esto es así porque, a diferencia de otros métodos (véanse los mencionados sobre flujos), una aproximación centrada en el comportamiento de los hosts
suele ser insensible a las variaciones que puedan presentar parámetros como los tiempos de llegada entre paquetes.
En cuanto a resultados, sobre sistemas de detección de anomalías se suelen aplicar enfoques de aprendizaje automático basados en patrones de comunicación entre los equipos informáticos.
Estos alcanzan resultados comparables a los de técnicas de DPI sobre firewalls, siendo notablemente más asequibles y menos invasivos con la privacidad.

Es por todo lo anterior que en los sistemas de detección de anomalías, que se van a desarrollar en la siguiente sección, priman los enfoques sobre el equipo informático final en vez de sobre el flujo.

\section{Detección de anomalías sobre actividad de red}\label{detectanomsobreactividadred}

En la gestión y monitorización de una red empresarial cobra especial relevancia la seguridad.
En este ámbito, la seguridad informática se centra en proteger la red corporativa de ataques que puedan comprometer su disponibilidad o la integridad de los equipos que la componen, así como
bloquear acciones no autorizadas y evitar el uso indebido de los recursos que quedan expuestos al exterior.

Las organizaciones toman numerosas medidas de seguridad frente a estas amenazas, tanto \emph{software} como \emph{hardware}.
Dos ejemplos claros serían los antivirus y los firewalls, que podríamos englobar dentro de las aproximaciones a la seguridad basadas en firmas \cite{Alconzo_2019}.
Sin embargo, estos métodos dependen de que el fabricante del producto de seguridad haya detectado el ataque previamente, haya generado una firma que lo identifique y haya distribuido la misma hasta el cliente final.
Es decir, solo pueden ofrecer protección ante ataques conocidos y requieren que todos los pasos anteriores se hayan completado.

En contraposición, existen los sistemas de seguridad basados en detección de anomalías.
Este tipo de métodos asumen que el impacto de un ataque modificará el comportamiento de la red, así que construyen un modelo que represente el comportamiento normal de la red, especificado por ciertas métricas.
A continuación, monitorizan el tráfico y fijan alarmas que se dispararán cuando el valor recogido en alguna de esas métricas de referencia se desvíe del rango considerado normal \cite{Boutaba_2018}.

Habitualmente, este tipo de defensas basadas en detección de anomalías son complementarias a las basadas en firmas.
Se sitúan en una segunda línea con el objetivo de detectar a tiempo síntomas tempranos de ciberataques de tipo \emph{zero-day}, para así poder actuar antes de que causen daños.
Ambos enfoques pueden encontrarse integrados en soluciones conocidas como IDS/IPS (\emph{Intrusion Detecion/Prevention System}).

Hablando en términos generales, pueden distinguirse tres fases básicas que cumplen todos los NIDS (\emph{Network} IDS) basados en anomalías \cite{GarciaTeodoro_2009}:
\begin{itemize}
    \item \emph{Parametrización}: las instancias del sistema objetivo se representan de forma adecuada para su tratamiento.
    \item \emph{Entrenamiento}: se caracteriza el comportamiento normal del sistema, mediante un modelo que puede construirse con técnicas basadas en alguna de las categorías descritas después.
    \item \emph{Detección}: se compara el modelo con el tráfico disponible, de forma que se dispara una alarma si una instancia se desvía.
\end{itemize}

Según cómo se modele el comportamiento normal del sistema \cite{Lazaveric_2005}, las técnicas pueden categorizarse en: basadas en estadística, en conocimiento o en aprendizaje automático.
Las primeras no requieren un conocimiento previo sobre la actividad normal del sistema, pero la presunción que asumen de cuasi-estacionalidad es poco realista.
Las segundas son robustas, pero el mantenimiento de datos de calidad resulta difícil y costoso.
En cuanto a las técnicas basadas en aprendizaje automático, son flexibles y adaptables.
También pueden capturar interdependencias entre las variables que no son fáciles de encontrar de otra forma.
No obstante, estas técnicas tienen una dependencia importante de lo que se acepte como comportamiento normal.

Para este trabajo, se ha elegido desarrollar un sistema de detección de anomalías basado en una técnica de aprendizaje automático como es el \emph{clustering}.
En \cite{Alconzo_2019} se resaltan acertadamente las bondades y debilidades de los métodos de detección de anomalías, al decir que
``son atractivos porque permiten la pronta detección de amenazas desconocidas (por ejemplo, \emph{zero-days}).
Estos métodos, sin embargo, puede que no detecten ataques sigilosos, insuficientemente amplios para perturbar la red.
A veces también adolecen de un alto número de falsos positivos.''

Continúa señalando cómo beneficia el aprendizaje automático a este tipo de sistemas: ``el \emph{machine learning} ha recibido una significativa atención en la detección de anomalías, debido a
la autonomía y robustez que ofrece en el aprendizaje y también a la hora de adaptar el perfil de la normalidad según va cambiando.
Con \emph{machine learning}, el sistema puede aprender patrones de comportamientos normales dentro de entornos, aplicaciones, grupos de usuarios y a lo largo del tiempo.
Además, ofrece la capacidad de encontrar correlaciones complejas en los datos que no pueden deducirse de la mera observación''.

Se concluye por tanto que la obtención de una representación completa de la normalidad, requisito no trivial en estos sistemas basados en detección de anomalías,
puede tomarse como un problema de clasificación en instancias normales y no normales.
Dicho problema puede abordarse mediante la técnica de aprendizaje no supervisado descrita a continuación.

\section{Clustering}\label{clustering}

El \emph{clustering} se define en \cite{NA08} como la agrupación de instancias que tienen características \emph{cercanas} en forma de \emph{clusters}, sin aplicar ninguna orientación previa.
Esta técnica de aprendizaje automático no supervisado asocia a las instancias con propiedades similares bajo el mismo grupo,
determinando dicha similaridad en un modelo que posibilite la medición de distancias específicas, como pueda ser el espacio euclídeo.
Los grupos pueden ser exclusivos, si cada instancia pertenece a un único grupo;
solapados, si una instancia puede pertenecer a varios grupos;
o probabilísticos, si la pertenencia de una instancia a un grupo se expresa mediante una cierta probabilidad.

El primer uso de \emph{clustering} para detección de intrusiones se vio en \cite{Portnoy_2000}.
La hipótesis en base a la cual los autores aplicaron \emph{clustering} para esta tarea es que las conexiones entre datos normales crearán \emph{clusters} más grandes y más densos.
Si se lleva el análisis un paso más allá, para incrementar la precisión de la técnica, también debe tenerse en cuenta la distancia entre \emph{clusters}, como se demuestra en \cite{JSW+06}.

Los tipos de algoritmos de \emph{clustering} usados en clasificación de tráfico son variados.
Encontramos en \cite{MHL+04} la primera aplicación de un algoritmo de \emph{clustering} probabilístico como es \emph{Expectation Maximization} sobre flujos de tráfico.
Considerando varias estadísticas sobre longitud de paquetes, tiempos entre llegadas, cantidad de bytes, duración de la conexión y
número de permutaciones entre conexiones de tipo ``transaccional'' y de tipo ``por lotes'', se consigue una clasificación de aplicaciones con baja granularidad.
En \cite{ZNA05}, se usan estadísticas similares y un método de selección de características para encontrar el conjunto de características óptimo
que aplicar a la entrada del algoritmo de \emph{clustering} ``AutoClass''.
Así lograron diferenciar con alta granularidad entre ocho aplicaciones previamente estudiadas, demostrando una precisión media del 86\%.

Otros estudios valoraban muchas menos variables.
En \cite{BTA+06}, tan solo se miraban la longitud y dirección de los primeros cinco paquetes de cada flujo (excluyendo el \emph{handshake} y los ACKs sin \emph{payload})
para decidir mediante K-Means a cuál de las diez aplicaciones tipo pertenecía cada flujo.
Permitía una temprana identificación de la aplicación con una precisión cercana al 80\%, extensible hasta el 85\% con mejoras posteriores \cite{BTS06} \cite{BT07}.

Para la detección de anomalías también se usan habitualmente algoritmos de \emph{clustering}.
Se advierte en \cite{Leung_2005} del alto coste de tener datos etiquetados a priori,
por lo que se aborda la detección de anomalías mediante técnicas de aprendizaje automático no supervisado.
Esta decisión se fundamenta en dos presunciones sobre los datos.
La primera, que la mayoría de las conexiones de red serán tráfico normal, y solo un pequeño porcentaje del tráfico será malicioso \cite{Portnoy_2000}.
La segunda, que el tráfico de un ataque será estadísticamente distinto al tráfico normal \cite{Javitz_1993}.
Estos trabajos citados, junto con otros muchos como los que se recogen y comparan entre sí en \cite{Bhuyan_2014} a través de varias medidas de validez de los \emph{clusters},
demuestran la utilidad de la técnica del \emph{clustering} en esta tarea.

Comprobada la idoneidad del \emph{clustering} para detectar anomalías, en \cite{Syarif_2012} se prueban varios algoritmos de \emph{clustering} frente a varios algoritmos clasificadores.
Se resalta que, como se ha apuntado anteriormente, los algoritmos clasificadores (como aprendizaje automático supervisado que son) son incapaces de detectar formas de intrusión o ataques no identificados previamente.
Se elaboraron experimentos sobre un conocido \emph{dataset} (el \emph{dataset} de detección de intrusiones de la KDD Cup '99).
Sus resultados revelaron que las técnicas de \emph{machine learning} supervisado testeadas (Naive Bayes, reglas de inducción, árboles de decisión y KNN) no alcanzaban el 64\% de precisión ante intrusiones desconocidas.
Por el contrario, las técnicas de \emph{machine learning} no-supervisado lograron tasas de detección del 80\%, aunque los falsos positivos superaban el 20\%.
Estas técnicas eran los siguientes algoritmos de \emph{clustering}: K-Means, \emph{k-medoids} \cite{Velmurugan_2010}, \emph{Expectation Maximization} \cite{Lu_2009} y detección de \emph{outliers} basada en distancia \cite{Orair_2010}.

Resumiendo, se ha constatado que el aprendizaje automático es adecuado para la clasificación de tráfico.
Basarlo en el comportamiento del host tiene en cuenta las acciones con el mismo origen, además de ser resistente a cambios de red.
Por último, en el ámbito de la seguridad, la detección de anomalías con aprendizaje automático es flexible y adaptable, puede capturar interdependencias entre variables y permite la pronta detección de \emph{zero-days}.
