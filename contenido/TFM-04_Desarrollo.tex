\chapter{Desarrollo}\label{chap:desarrollo}
\textbf{Desarrollo específico de la contribución}

%Párrafo introductorio del capítulo
[Párrafo introductorio del capítulo. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Natus impedit sint cumque, omnis assumenda, molestias corporis repellat, reprehenderit, ullam labore aliquam. Velit ut, ab amet a recusandae, eaque similique alias!]

\section{Presentación del escenario}\label{sec:presentaciondelescenario}

En el escenario del proyecto (la red de un banco que es cliente de la empresa), el tráfico atraviesa distintos equipos de seguridad según el entorno al que corresponda.
Si se trata de tráfico que procede de los usuarios externos conectándose a servicios públicos del banco (expuestos a Internet), se cursa a través de una batería de equipos formada por:
un IPS\footnote{\emph{Intrusion Prevention System}} PaloAlto, un WAF\footnote{\emph{Web Application Firewall}} de marca Fortinet, balanceadores F5, otros firewalls Checkpoint y, finalmente, los servidores.
En caso de ser tráfico desde la red corporativa hacia Internet, los equipos en este camino son el IPS PaloAlto mencionado (pero en esta ocasión está funcionando solo como IDS\footnote{\emph{Intrusion Detection System}}) y un firewall Fortinet, en este orden.
Si son conexiones internas entre diferentes organizaciones, se procesa con otro firewall Fortinet independiente, etc.

\begin{figure}[h]
    \centering
    \captionsetup{width=10cm}
    \includegraphics[width=12cm]{contenido/fig/diagrama_red.pdf}
    \caption{Diagrama de equipos de red}
    \label{fig:diagramared}
\end{figure}

El objetivo de este trabajo se centra en la clasificación de los equipos de la red empresarial, así que se pondrá la atención sobre el segundo itinerario descrito: el tráfico desde la red hacia Internet.
En especial, se quiere exprimir la información que proporciona el firewall Fortinet de ``Internet Corporativo'', ya que es principalmente quien reacciona con mecanismos de prevención o bloqueos ante las acciones iniciadas en la red corporativa.
Como se ha explicado, al estar el IPS actuando como IDS en este ámbito y además procesando tráfico de otras redes y en otras direcciones, su información nos interesa menos.
Sin embargo, si en un momento dado del desarrollo de este método de \emph{clustering} se cree beneficioso incorporar los eventos que arroja el IPS sobre las sesiones de los hosts de la red,
será necesario que se hayan analizado correctamente y puedan extraerse con facilidad.
Por eso, en la siguiente sección se detalla cómo se han estudiado los logs de ambos.

\section{Extracción y filtrado}\label{sec:extraccionyfiltrado}

El punto de interés para la captura se concentra por tanto en dos equipos por los que pasa el grueso del tráfico total: un firewall Fortinet y un IPS PaloAlto.
Como es habitual, estos equipos reportan todas sus acciones a través de logs, con varios niveles de granularidad e importancia.
Incluyen también multitud de información aportada por los propios sistemas que enriquecen el valor de cada evento.
Esto es razón para preferir los logs de firewall como fuente de información frente a una captura de tráfico en crudo, ya que
lo que procesan los firewalls casi siempre es más relevante que el tráfico completo pero sin procesar.
Además, el volumen de una captura de tráfico sería notablemente mayor y más difícil de manejar.

\begin{figure}[h]
    \centering
    \captionsetup{width=12cm}
    \includegraphics[width=9cm]{contenido/fig/volumen_logs.pdf}
    \caption{Volumen de logs en bruto y tras ser procesados, en una semana}
    \label{fig:volumenlogs}
\end{figure}

La cantidad de datos recibida en los logs sigue siendo alta, pero necesita ser procesada para obtener datos útiles.
En los párrafos siguientes se explica cómo se han extraído los datos de sesión que se usarán como materia prima para tener finalmente unos datos de entrada al algoritmo de \emph{clustering}.
Se ha tomado una semana de muestra para trabajar con un periodo acotado.
Nótese en \ref{fig:volumenlogs} la reducción de volumen en sucesivas etapas hasta aislar únicamente información valiosa.

Aunque el fin para el que sirven ambos equipos (análisis y protección frente a amenazas informáticas) sea similar,
la estructura usada por cada uno en los logs que produce es completamente distinta.
Los logs de Fortinet siguen un formato clave-valor con ciertas particularidades, mientras que los de PaloAlto tienen una serie de campos fijos que están delimitados por comas.
A modo de ejemplo, las dos líneas adjuntadas a continuación corresponden a un evento del firewall Fortinet y otro del IPS PaloAlto, respectivamente (cada evento viene en una única línea):

\begingroup
\makeatletter
\@totalleftmargin=-1cm
\begin{verbatim}

1585572524|1585572524|2020-03-30T06:48:44.202297|10.2.0.11|6|local7|
date=2020-03-30 time=06:48:44 devname="FW1_INTERNETCORP" devid="FG1809999"
logid="1059028704" type="utm" subtype="app-ctrl" eventtype="app-ctrl-all"
level="information" vd="root" eventtime=1585572524 appid=41470 user="NOM"
group="GrupoOffice365" authserver="SV1" srcip=172.2.9.6 dstip=23.203.51.72
srcport=54697 dstport=443 srcintf="p18" srcintfrole="undef" dstintf="p20"
dstintfrole="wan" proto=6 service="HTTPS" direction="outgoing" policyid=124
sessionid=325186437 applist="AC_CORREO" appcat="Collab" app="Microsoft.CDN"
action="pass" hostname="img-prod-cms-rt-microsoft-com.akamaized.net"
incidentserialno=1513678724 url="/" msg="Collaboration: Microsoft.CDN,"
apprisk="elevated" scertcname="a248.e.akamai.net"

1585659863|1585659863|2020-03-31T07:04:23.027791|10.2.0.73|6|local0|
1,2020/03/31 07:04:23,001801037558,TRAFFIC,end,2049,2020/03/31 07:04:03,
10.138.4.7,186.151.236.155,0.0.0.0,0.0.0.0,OUTBOUND,,,incomplete,vsys1,
trust,untrust,ethernet1/10,ethernet1/9,Log-Panorama,2020/03/31 07:04:03,
41602,1,55074,80,0,0,0x19,tcp,allow,132,132,0,2,2020/03/31 07:03:55,3,any,
0,1307298109,0x80000,10.0.0.0-10.255.255.255,America,0,2,0,aged-out,13,0,0,
0,,PA-3020-Z9,from-policy,,,0,,0,,N/A,0,0,0,0

\end{verbatim}
\endgroup

Otro hecho reseñable que afecta al formato es que se emplea \emph{syslog}\footnote{\url{https://tools.ietf.org/html/rfc5424}} (el estándar de facto)
como protocolo para trasladar los datos desde cada equipo hasta el punto de recolección,
de forma que se cuenta con ciertos campos adicionales a los enviados por los equipos.
Para el tema que nos ocupa, los únicos campos que se extraen de esta cabecera son:
la marca de tiempo en la que ha llegado cada evento, conocida en el vocabulario informático como \emph{timestamp}, y lo que conoceremos como la prioridad del evento
(que en la especificación de \emph{syslog} se denomina severidad, pero se ha creído que el término ``prioridad'' es más acertado en este entorno).
En cualquier caso, esta sección adicional dentro de los logs tiene también su propio formato, por lo cual también se deberá tratar de forma específica.
En nuestra configuración (que aplica a la herramienta \emph{rsyslog}\footnote{\url{https://www.rsyslog.com/}}),
la siguiente directiva establece cómo se vuelcan a fichero estos campos de syslog:

\begin{verbatim}
template(name="FORMATO_LOGS" type="string"
string="%timereported:::date-unixtimestamp%
    |%timegenerated:::date-unixtimestamp%
    |%timegenerated:::date-rfc3339%|%fromhost-ip%
    |%syslogseverity%|%syslogfacility-text%| %syslogtag%%msg%\n")
\end{verbatim}

Así que, en los \emph{scripts} que procesan los ficheros donde se han volcado los datos traídos mediante \emph{syslog},
se obtiene la fecha de cada evento a partir de este primer campo ``timereported:::date-unixtimestamp'' y la prioridad a partir del quinto campo.
Esta primera parte del procesado (que está programado en Python) se hace de la siguiente manera:

\begin{minted}{python}
for syslogline in sys.stdin:

    try:

        splitted_syslogline = syslogline.rstrip().split("|") # .rstrip() removes last "\n" character

        tstamp_line = int(splitted_syslogline[0])

        prio = splitted_syslogline[4]
\end{minted}

Seguidamente, el resto de la línea actual (sin la cabecera de \emph{syslog}) se convierte en una estructura de diccionario.
Como se apreciaba en las líneas de ejemplo que se han incluido antes, la relación entre claves y valores depende de cada caso.

Para el equipo Fortinet, la relación está definida en el propio evento como \texttt{clave="valor"} o \texttt{clave=valor} para valores no considerados como cadenas de texto.
Cabe destacar que el símbolo ``\texttt{=}'' puede estar contenido en el valor.
El nombre de la clave, sin embargo, nunca llevará comillas.
Establecidas las anteriores reglas, en Fortinet se convierten los campos con una expresión regular y una \emph{dict comprehension}\footnote{\url{https://www.python.org/dev/peps/pep-0274/}}
(es decir, una forma concisa de crear diccionarios a través de la iteración sobre una lista con la posibilidad de incluir condicionales):

\begin{minted}{python}
    line = "".join(splitted_syslogline[6:]) # removes "1581410810|1581410810|2020-02-11T02:46:51.421302|10.25.0.6|5|local7|"

    fields = re.split("([^ \"]+=[^ \"]+)|([^ \"]+=\"[^\"]+\")", line)

    dict_line = {k:v.strip('"')
                 for k,v in [f.split("=", 1)
                 for f in fields if (f and "=" in f)]}
\end{minted}

Para el IPS PaloAlto, la extracción de los campos es más sencilla.
Como cumplen con el formato CSV estandarizado\footnote{\url{https://tools.ietf.org/html/rfc4180}}, los valores se tienen en una lista
con solo leer la línea a través de una función de la librería \texttt{csv}.
En cuanto a las claves, en la documentación\footnote{\url{https://docs.paloaltonetworks.com/pan-os/8-1/pan-os-admin/monitoring/use-syslog-for-monitoring/syslog-field-descriptions.html}}
de PaloAlto se explica que depende del tipo del evento.
Por tanto, se asignan unas claves u otras consultando primero de qué tipo se trata.
Finalmente, se construye el diccionario con otra \emph{dict comprehension}:

\begin{minted}{python}
    line = "".join(splitted_syslogline[6:]) # removes "1581410810|1581410810|2020-02-11T02:46:51.421302|10.25.0.6|5|local7|"

    values = list( csv.reader([line]) )[0]

    this_type_keys = []

    if values[3]=="TRAFFIC": # type is on 4th field
        this_type_keys.extend(common_trafficthreat_fields)
        this_type_keys.extend(traffic_fields)
    elif values[3]=="THREAT":
        ...

    if this_type_keys!=[]:
        dict_line = {k:v for k,v in zip(this_type_keys,values)}
\end{minted}

Posteriormente se lleva a cabo otra serie de operaciones necesarias para la transformación de los datos de entrada en información útil para la monitorización.
Sin embargo, desde la perspectiva de este trabajo, el único apartado de interés es la agregación de sesiones, que se desarrolla a continuación.

Una parte de este procesado consiste en guardar cierta información asociada a cada sesión.
El concepto de ``sesión'' sería equivalente al de ``flujo'' presentado en el \hyperref[chap:estadodelarte]{capítulo anterior}:
una serie de eventos asociados que se corresponden con la misma tupla de \{IP origen, IP destino, protocolo, puerto origen, puerto destino\}.
Los dos equipos mantienen un campo ``Identificador de Sesión'' interno que se añade a la tupla de la sesión.
Este campo permite distinguir los eventos de sesiones que coinciden en origen y destino pero se producen en intervalos temporales diferentes.
Se incluye en el procesado con esta finalidad de no confundir sesiones distintas en etapas posteriores, pero para nuestro propósito de clasificación de equipos puede ignorarse.

La información de cada sesión se compone de:
\begin{itemize}
    \item Tupla que define la sesión:\\\{ID de sesión, IP origen, IP destino, protocolo, puerto origen, puerto destino\}
    \item Timestamps del primer y último evento pertenecientes a esta sesión
    \item Máxima prioridad de evento vista en esta sesión
    \item Bytes recibidos y enviados (solo en el IPS)
    \item Nivel de anomalía
    \item Nivel de amenaza
    \item Contador y lista de eventos
\end{itemize}

Los niveles de anomalía y amenaza son unos índices simples que se han diseñado para resumir cualidades de interés acerca de la sesión, como son
cuánto se aleja de la normalidad la cantidad de eventos prioritarios que se han visto y cómo son de peligrosas las amenazas recibidas.
Para cada sesión, se calculan sus niveles con las fórmulas:

\begin{eqnarray*}
    & \mathlarger{\mathlarger{\mathlarger{N}}}_{\textrm{anomalía}} = \mathlarger{\mathlarger{\mathlarger{\sum}}}_{i=1}^{n} \frac{1}{\textrm{prioridad}_{\textrm{evento}_i}}\\
    & \textrm{para eventos de prioridad }\leq4\textrm{ o eventos de tráfico que no son de inicio ni fin}
\end{eqnarray*}

\begin{eqnarray*}
    & \mathlarger{\mathlarger{\mathlarger{N}}}_{\textrm{amenaza}} = \mathlarger{\mathlarger{\mathlarger{\sum}}}_{i=1}^{n} \frac{1}{\textrm{prioridad}_{\textrm{evento}_i}}\\
    & \textrm{para eventos de amenaza con prioridad }\leq4\textrm{ que no son bloqueados}
\end{eqnarray*}

Tanto estas como el resto de características cuantificables pueden resultar de interés a la hora de aplicar \emph{clustering} sobre un \emph{dataset} derivado de este procesado.

La recolección de esta información se realiza a través de la función adjunta.
Como puede verse, cuando se llama a esta función (lo cual ocurre ante todos los eventos de protocolo TCP o UDP que incluyan ID de sesión)
se actualizan los parámetros relativos a la sesión actual, que están almacenados en un diccionario.
A su vez, este diccionario se encuentra dentro del diccionario \texttt{sessions}.
Con él se mantienen en memoria todas las sesiones que todavía no se han cerrado.
Cuando transcurre un \emph{bucket} de tiempo determinado (por defecto, 60 segundos), se comprueban todas las sesiones vigentes.
Aquellas que tienen un evento de finalización se imprimen en un fichero y se retiran del diccionario \texttt{sessions}.
De este modo, cada 60 segundos se tienen nuevas sesiones completas (en la práctica se alcanzan incluso más de 100 000 sesiones finalizadas cada minuto)
que serán datos de entrada para el \emph{clustering}.

\begin{minted}{python}
def session_aggregation(dict_line, event_descript, event_tstamp):
    """
    It groups info related to actual session on the sessions dictionary, that is:
    - the sessionid and the session tuple (srcip-dstip-proto-srcport-dstport)
    - tstamp of first and last event observed for actual session
    - update max. priority of events observed for actual session
    - sent and received bytes for actual session
    - counter and list of events observed for actual session
    - recalculate anomaly level for actual session
    - recalculate threat level for actual session
    """

    session_tuple = "···".join([
            dict_line['SESSION ID'], dict_line['SRC_IP'], dict_line['DST_IP'],
            dict_line['PROTO'], dict_line['SRC_PORT'], dict_line['DST_PORT']
        ])

    priority = int(dict_line['priority'])

    if session_tuple not in sessions:
        sessions[session_tuple] = {}
        # store the session tuple values related to this new session_tuple:

        sessions[session_tuple]['events'] = []
        sessions[session_tuple]['anomaly_level'] = 0
        sessions[session_tuple]['threat_level'] = 0
        sessions[session_tuple]["counter"] = 0
        sessions[session_tuple]['max_prio'] = priority
        sessions[session_tuple]["bytes_sent"] = 0
        sessions[session_tuple]["bytes_rcvd"] = 0
        sessions[session_tuple]['first_event_tstamp'] = int(event_tstamp)

    sessions[session_tuple]['last_event_tstamp'] = int(event_tstamp)
    sessions[session_tuple]["counter"] += 1

    if dict_line['type']=="TRAFFIC":
        sessions[session_tuple]["bytes_sent"] += int(dict_line['BYTES_SENT'])
        sessions[session_tuple]["bytes_rcvd"] += int(dict_line['BYTES_RECEIVED'])

    if priority < sessions[session_tuple]['max_prio']:
    # lower prio value means more important (i.e., the most important priority is 1, or even 0 if priority=0 exists)
        sessions[session_tuple]['max_prio'] = priority
    else:
        sessions[session_tuple]['max_prio']

    if priority<=4 or (dict_line['type']=="TRAFFIC" and "end" not in event_descript and "start" not in event_descript):
        sessions[session_tuple]['anomaly_level'] += 1/priority
    if priority<=4 and dict_line['type']=="THREAT" and dict_line['ACTION']!="alert":
        sessions[session_tuple]['threat_level'] += 1/priority

    sessions[session_tuple]['events'].append("{}, {}".format(event_descript, dict_line['ACTION']))
\end{minted}

\section{Preprocesado para el clustering}\label{sec:preprocesado}

A partir de los datos de sesiones finalizadas descritos anteriormente, se debe obtener un \emph{dataset} en el formato adecuado para aplicar algoritmos de \emph{clustering} sobre él.
Ya se ha señalado que el volumen de datos es muy alto, así que como primera aproximación se extraerá una muestra suficientemente grande pero operable a priori.
También se empezará por los datos de sesiones vistas en el Fortinet, ya que, al ser el equipo que vigila las conexiones desde la red corporativa a Internet,
resulta de más interés y presumiblemente tendrá una actividad más relevante de cara a clasificarla.

La cantidad de sesiones diaria en el firewall Fortinet suele estar en torno a los 20 millones.
Se practica un muestreo aleatorio simple que reduce los datos a un 5\%,
de forma que el tamaño de la muestra (1 millón de sesiones) sea significativo para obtener unas primeras conclusiones pero su tratamiento no sea excesivamente costoso.

Los datos se guardan en un fichero por día, rotándose a diario y conservándose así (en texto plano, dispuestos para ser importados a una base de datos) durante 14 días antes de eliminarse.
Mediante el siguiente comando, se copian 1 millón de sesiones aleatorias, formadas por
\{IP origen, IP destino, protocolo, puerto origen, puerto destino, nivel de anomalía, nivel de amenaza, máxima prioridad, cantidad de eventos\},
y se repite para los 7 ficheros de una semana:

\begin{verbatim}
$ shuf -n 1000000 FORTINET_INTERNET_CORP_10.251.0.101.1 | \
    awk -F"···" '{print $5,$6,$7,$8,$9,$10,$11,$12,$13}' \
    > datasets_clustering/FORTINET_INTERNET_CORP_10.251.0.101.23may
\end{verbatim}

El siguiente paso consiste en agrupar los datos por IP de origen, que será la característica identificativa de cada \emph{datapoint}.
Para cada valor de IP de origen, se contabilizan cuántos valores distintos se observan en el resto de variables.
Como resultado de esta transformación, se tiene una matriz con 9 columnas y \emph{n} filas, siendo \emph{n} el número de IPs de origen distintas presentes en la muestra de ese día.
Cada valor de esta matriz resume las sesiones que un host (identificado por su IP de origen) ha mantenido, a través de las siguientes métricas:

\begin{verbatim}
len(data[src_ip]['dst_ip']),       len(data[src_ip]['proto']),
len(data[src_ip]['src_port']),     len(data[src_ip]['dst_port']),
mean(data[src_ip]['anom_level']),  mean(data[src_ip]['threat_level']),
min(data[src_ip]['max_prio']),     sum(data[src_ip]['count_events'])
\end{verbatim}

Es decir, las cuatro primeras variables cuentan el número de IPs destino, protocolos, puertos origen y puertos destino que corresponden a una IP origen.
Las dos variables siguientes, que son los niveles de anomalía y amenaza, se hallan calculando la media de cada nivel.
La variable ``máxima prioridad'' corresponde al valor de prioridad más bajo de un evento visto para esta IP origen.
Nótese que los niveles de proridad en \emph{syslog} (más precisamente, lo que \emph{syslog} llama ``severidad'') son descendentes,
siendo 0 el nivel de emergencia, 1 el nivel de alerta, 2 el nivel crítico, 3 error, 4 peligro, 5 aviso, 6 información y por último, como menos importante, 7 depuración.
Por tanto, respetando la definición original de estos niveles de prioridad de los eventos,
se debe tomar como prioridad de máxima importancia aquella prioridad de evento mínima de todas las vistas con esta IP origen.
Finalmente, la variable \texttt{count\_events} es la suma de eventos que el firewall ha asociado a una misma IP origen.

Procediendo de la manera detallada hasta ahora sobre los datos recogidos entre el lunes 25 de mayo de 2020 y el domingo 31,
se tienen 7 matrices de 9 variables por unas 4800 filas (equivalentes al total de IPs origen distintas) cada día,
apreciándose un descenso de la cantidad de orígenes únicos en torno al fin de semana
(4700 filas distintas el viernes y alrededor de 3000 filas tanto sábado como domingo).
En total, se dispone de prácticamente 30 000 muestras.
Con este material se puede abordar el prototipado del método de \emph{clustering} sobre el que se fundamenta este proyecto.

[Anonimizar IPs? Puedo usar las funciones de \cite{Labayen_2020}]

\section{Análisis de datos}\label{sec:analisisdedatos}

Una vez obtenidas las características, se pasa a analizar la distribución de valores que presenta cada una de ellas en este \emph{dataset}.
Dicha tarea permitirá entender mejor la importancia relativa de cada característica y cómo afectará a los algoritmos de \emph{clustering}.
En concreto, se prestará especial atención a la forma, varianza y modalidades de las distribuciones.

[Figura: Funciones de distribución acumulada para cada característica]

\section{Selección de características}\label{sec:selecciondecaracteristicas}

Como se apunta en ``Análisis de las características en tráfico de red para detección de anomalías'' \cite{Iglesias_2015},
la meta que tiene la selección de características en la detección de anomalías es
``eliminar características fuertemente correladas, redundantes e irrelevantes para mejorar la calidad de la detección''.
En este trabajo, los autores abordaron la selección de características de forma exhaustiva y rigurosa,
mediante métodos multi-fase implementados con envolvedores (\emph{wrappers}, que buscan el subconjunto de características con mejores resultados),
combinando filtrado y técnicas de regresión gradual.
Para nuestro caso, se ha optado por un procedimiento más sencillo, fundamentado en dos factores: la correlación entre características y la aportación de cada una al agrupamiento.

De acuerdo con la argumentación de \cite{Bohara_2016}, si las características de un conjunto están altamente correladas,
contendrán información redundante y provocarán un incremento innecesario de la complejidad del algoritmo.
Para evitar esta redundancia, en dichos conjuntos fuertemente correlados se seleccionará una única característica.
La correlación puede determinarse empleando el coeficiente de correlación de Pearson.
Se decide que dos características se tomarán como fuertemente correladas si su coeficiente de correlación es mayor de 0,99.
Ya que se tiene un \emph{dataset} de tamaño bastante grande y el rango de valores de las características está acotado,
se espera que el coeficiente de correlación de Pearson proporcione una estimación de la correlación suficientemente precisa.

El otro aspecto a considerar es la contribución de cada característica en la fase de \emph{clustering}.
Si, al analizar el vector de los pesos relativos que tiene cada característica sobre cada uno de los \emph{clusters},
se aprecia que ciertas características tienen un papel poco activo en la clasificación,
se simplificará el set de características eliminando aquellas que aportan débilmente al algoritmo.

Es importante distinguir la sutil diferencia entre selección de características y extracción de características.
Ambos son métodos para reducir la dimensionalidad, pero el proceso que siguen es distinto.
La selección de características descarta características irrelevantes o redundantes para los procesos posteriores de representación y clasificación de datos.
En cambio, la extracción de características consiste en proyectar el \emph{dataset} original en un nuevo espacio vectorial
donde se minimice la dependencia lineal entre características, reduciendo por tanto el número de variables necesarias.
Aunque estas técnicas (por ejemplo, el análisis de componentes principales o PCA) permiten ponderar la importancia de las características y por tanto seleccionarlas,
debe destacarse que solo consideran relaciones lineales entre variables (ignorando cualquier otro tipo de interacción entre ellas).
Por eso se puede afirmar que no es un método ideal para seleccionar características, si bien es cierto
que pueden combinarse selección y extracción de características, eliminando características irrelevantes primero y proyectándolas en espacios optimizados después.

También se valora la observación de \cite{Guyon_2003} cuando dice que ``el objetivo de la selección de variables es triple: mejorar el rendimiento de los predictores, posibilitar predictores más rápidos y eficientes en coste, y permitir una mejor comprensión del proceso subyacente que ha generado los datos''.
La selección de características aquí expuesta busca cumplir con estas tres finalidades.

[Explicar proceso de selección (filtrar feats. correladas y quedarse con las de más peso)]

\section{Parametrización}\label{sec:parametrizacion}
